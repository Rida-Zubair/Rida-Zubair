{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rida-Zubair/Rida-Zubair/blob/main/i23_2590_Assignment1_DS_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWYhG4BxhxGH"
      },
      "source": [
        "# **Important Rules and Restrictions**\n",
        "\n",
        "Use of the following libraries is strictly prohibited:\n",
        "\n",
        "- NLTK  \n",
        "- spaCy  \n",
        "- HuggingFace Tokenizers  \n",
        "- SentencePiece  \n",
        "- Urduhack  \n",
        "- Stanza  \n",
        "- Polyglot  \n",
        "- Gensim Language Models  \n",
        "- sklearn CountVectorizer / TfidfVectorizer  \n",
        "\n",
        "Violation will result in zero marks.\n",
        "\n",
        "### **Allowed Libraries**\n",
        "Students may use:\n",
        "\n",
        "- Python Standard Functions\n",
        "- Regex  \n",
        "- NumPy  \n",
        "- Pandas  \n",
        "- BeautifulSoup / Requests / Scrapy / Selenium (For use of Scraping only)  \n",
        "- Matplotlib  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmdkf-JthxGa"
      },
      "source": [
        "## **Part 1: BBC Urdu Dataset Collection and Preprocessing**\n",
        "In this part, BBC Urdu news articles are collected and prepared into a clean dataset using normalization and custom preprocessing tools. The output of this part will be reused directly in Part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh_mh-aEhxGd"
      },
      "source": [
        "News articles must be scraped from:\n",
        "https://www.bbc.com/urdu\n",
        "\n",
        "Students must scrape:\n",
        "- Minimum: 200 articles\n",
        "- Maximum: 300 articles\n",
        "\n",
        "Each article must be complete and properly structured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h14f9jn5hxGe"
      },
      "source": [
        "Article metadata must be stored in a JSON file with the following constraints:\n",
        "- Each article must be numbered\n",
        "- Article numbers must be unique\n",
        "- Article numbers must match TXT files\n",
        "- Article body must not be included\n",
        "\n",
        "### ***Format Example***\n",
        "```json\n",
        "{\n",
        "  \"1\": {\n",
        "    \"title\": \"پاکستان میں مہنگائی کی شرح میں اضافہ\",\n",
        "    \"publish_date\": \"2024-01-15\"\n",
        "  },\n",
        "  \"2\": {\n",
        "    \"title\": \"کراچی میں بارش کے بعد صورتحال\",\n",
        "    \"publish_date\": \"2024-02-02\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOFE2-C-hxGh",
        "outputId": "30b0d2c8-68a8-47fa-f121-3cf83a83fc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting BBC Urdu article scraper...\n",
            "============================================================\n",
            "Strategy 1: Scraping from topic pages with pagination...\n",
            "  Topic c40379e2ymxt, page 1: 24 total links\n",
            "  Topic c40379e2ymxt, page 2: 48 total links\n",
            "  Topic c40379e2ymxt, page 3: 72 total links\n",
            "  Topic c40379e2ymxt, page 4: 96 total links\n",
            "  Topic c40379e2ymxt, page 5: 120 total links\n",
            "  Topic c40379e2ymxt, page 6: 144 total links\n",
            "  Topic c40379e2ymxt, page 7: 168 total links\n",
            "  Topic c40379e2ymxt, page 8: 192 total links\n",
            "  Topic c40379e2ymxt, page 9: 216 total links\n",
            "  Topic c40379e2ymxt, page 10: 240 total links\n",
            "  Topic c40379e2ymxt, page 11: 264 total links\n",
            "  Topic c40379e2ymxt, page 12: 288 total links\n",
            "  Topic c40379e2ymxt, page 13: 312 total links\n",
            "  Topic c40379e2ymxt, page 14: 336 total links\n",
            "  Topic ckdxnx900n5t, page 1: 355 total links\n",
            "  Topic ckdxnx900n5t, page 2: 373 total links\n",
            "  Topic ckdxnx900n5t, page 3: 395 total links\n",
            "  Topic ckdxnx900n5t, page 4: 419 total links\n",
            "  Topic ckdxnx900n5t, page 5: 443 total links\n",
            "  Topic ckdxnx900n5t, page 6: 467 total links\n",
            "  Topic ckdxnx900n5t, page 7: 491 total links\n",
            "  Topic ckdxnx900n5t, page 8: 515 total links\n",
            "  Topic ckdxnx900n5t, page 9: 539 total links\n",
            "  Topic ckdxnx900n5t, page 10: 563 total links\n",
            "  Topic ckdxnx900n5t, page 11: 587 total links\n",
            "  Topic ckdxnx900n5t, page 12: 611 total links\n",
            "\n",
            "Strategy 2: Scraping from date-based archives...\n",
            "\n",
            "Strategy 3: Following 'More Stories' and related links...\n",
            "\n",
            "============================================================\n",
            "Total unique article links found: 611\n",
            "============================================================\n",
            "\n",
            "Phase: Scraping articles (target: 300)...\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "Successfully scraped: 300 articles\n",
            "Failed/skipped: 6 articles\n",
            "============================================================\n",
            "\n",
            "✓ Saved metadata for 300 articles to metadata.json\n",
            "✓ Collected 300 article bodies\n",
            "\n",
            "Sample article:\n",
            "  Title: بیبی آئل، منشیات سے بھرا گوچی بیگ اور ’وفاداری کا امتحان‘: امریکی گلوکار ’ڈڈی‘ کی سیکس پارٹیوں میں ان کے عملے نے کیا کچھ دیکھا\n",
            "  Date: 2025-07-03\n",
            "  Body length: 12358 characters\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def scrape_bbc_urdu_articles(target_count=300):\n",
        "    \"\"\"Scrape BBC Urdu articles using multiple strategies\"\"\"\n",
        "    BASE_URL = \"https://www.bbc.com\"\n",
        "\n",
        "    article_links = set()\n",
        "    metadata = {}\n",
        "    raw_articles = {}\n",
        "\n",
        "    print(\"Strategy 1: Scraping from topic pages with pagination...\")\n",
        "\n",
        "    # BBC Urdu topics with potential pagination\n",
        "    topics = [\n",
        "        \"c2dwqd2ygdxt\",  # Pakistan\n",
        "        \"c06gq9v4xp3t\",  # World\n",
        "        \"c40379e2ymxt\",  # India\n",
        "        \"c9wpm0en87xt\",  # Sports\n",
        "        \"c2lej05epw5t\",  # Entertainment\n",
        "        \"ckdxnx900n5t\",  # Science & Tech\n",
        "        \"c4794229lwwt\",  # Health\n",
        "    ]\n",
        "\n",
        "    # Try to get articles from each topic with page parameter\n",
        "    for topic in topics:\n",
        "        for page in range(1, 15):  # Try up to 15 pages per topic\n",
        "            try:\n",
        "                # Try different URL patterns\n",
        "                urls_to_try = [\n",
        "                    f\"https://www.bbc.com/urdu/topics/{topic}?page={page}\",\n",
        "                    f\"https://www.bbc.com/urdu/topics/{topic}/page/{page}\",\n",
        "                ]\n",
        "\n",
        "                for url in urls_to_try:\n",
        "                    try:\n",
        "                        res = requests.get(url, timeout=15)\n",
        "                        if res.status_code != 200:\n",
        "                            continue\n",
        "\n",
        "                        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "                        # Find article links\n",
        "                        found_new = False\n",
        "                        for a in soup.find_all(\"a\", href=True):\n",
        "                            href = a['href']\n",
        "                            if \"/urdu/articles/\" in href:\n",
        "                                if href.startswith('/'):\n",
        "                                    link = urljoin(BASE_URL, href)\n",
        "                                else:\n",
        "                                    link = href\n",
        "\n",
        "                                if link not in article_links:\n",
        "                                    article_links.add(link)\n",
        "                                    found_new = True\n",
        "\n",
        "                        if found_new:\n",
        "                            print(f\"  Topic {topic}, page {page}: {len(article_links)} total links\")\n",
        "\n",
        "                        time.sleep(0.5)\n",
        "\n",
        "                        if not found_new:\n",
        "                            break  # No new articles, move to next topic\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if len(article_links) >= target_count * 2:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if len(article_links) >= target_count * 2:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nStrategy 2: Scraping from date-based archives...\")\n",
        "\n",
        "    # Try date-based URLs\n",
        "    if len(article_links) < target_count * 1.5:\n",
        "        today = datetime.now()\n",
        "        for days_back in range(0, 180, 7):  # Go back 6 months, weekly\n",
        "            date = today - timedelta(days=days_back)\n",
        "            date_str = date.strftime(\"%Y/%m/%d\")\n",
        "\n",
        "            try:\n",
        "                url = f\"https://www.bbc.com/urdu/{date_str}\"\n",
        "                res = requests.get(url, timeout=10)\n",
        "                if res.status_code == 200:\n",
        "                    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "                    for a in soup.find_all(\"a\", href=True):\n",
        "                        if \"/urdu/articles/\" in a['href']:\n",
        "                            link = urljoin(BASE_URL, a['href'])\n",
        "                            article_links.add(link)\n",
        "\n",
        "                    if len(article_links) % 50 == 0:\n",
        "                        print(f\"  Checked {days_back} days back: {len(article_links)} links\")\n",
        "\n",
        "                time.sleep(0.3)\n",
        "\n",
        "                if len(article_links) >= target_count * 2:\n",
        "                    break\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    print(f\"\\nStrategy 3: Following 'More Stories' and related links...\")\n",
        "\n",
        "    # Scrape a few articles to find related links\n",
        "    if len(article_links) < target_count * 1.5:\n",
        "        sample_articles = list(article_links)[:30]\n",
        "        for article_url in sample_articles:\n",
        "            try:\n",
        "                res = requests.get(article_url, timeout=10)\n",
        "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "                # Look for related articles sections\n",
        "                related_sections = soup.find_all(['div', 'section'],\n",
        "                    class_=re.compile(r'.*related.*|.*more.*|.*recommend.*', re.I))\n",
        "\n",
        "                for section in related_sections:\n",
        "                    for a in section.find_all(\"a\", href=True):\n",
        "                        if \"/urdu/articles/\" in a['href']:\n",
        "                            link = urljoin(BASE_URL, a['href'])\n",
        "                            article_links.add(link)\n",
        "\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                if len(article_links) >= target_count * 2:\n",
        "                    break\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"  Found {len(article_links)} total links through related articles\")\n",
        "\n",
        "    article_links = list(article_links)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total unique article links found: {len(article_links)}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Scrape articles\n",
        "    print(f\"Phase: Scraping articles (target: {target_count})...\\n\")\n",
        "\n",
        "    article_id = 1\n",
        "    failed_count = 0\n",
        "\n",
        "    for idx, link in enumerate(article_links):\n",
        "        if article_id > target_count:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            print(f\"Scraping {article_id}/{target_count} (tried {idx+1}/{len(article_links)})...\", end='\\r')\n",
        "            r = requests.get(link, timeout=15)\n",
        "            s = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "            # Extract title\n",
        "            title_tag = s.find(\"h1\") or s.find(\"h2\", class_=re.compile(r'.*title.*', re.I))\n",
        "            if not title_tag:\n",
        "                failed_count += 1\n",
        "                continue\n",
        "\n",
        "            title = title_tag.text.strip()\n",
        "\n",
        "            # Extract date\n",
        "            date_tag = s.find(\"time\")\n",
        "            publish_date = \"\"\n",
        "            if date_tag:\n",
        "                publish_date = date_tag.get('datetime', '') or date_tag.text.strip()\n",
        "\n",
        "            # Extract body\n",
        "            body_tags = []\n",
        "            article_body = s.find(['article', 'main'], class_=re.compile(r'.*article.*|.*story.*', re.I))\n",
        "            if article_body:\n",
        "                body_tags = article_body.find_all(\"p\")\n",
        "\n",
        "            if not body_tags:\n",
        "                body_tags = s.find_all(\"p\", class_=re.compile(r'.*text.*|.*paragraph.*', re.I))\n",
        "\n",
        "            if not body_tags:\n",
        "                body_tags = s.find_all(\"p\")\n",
        "\n",
        "            paragraphs = []\n",
        "            for p in body_tags:\n",
        "                text = p.text.strip()\n",
        "                if (len(text) > 30 and\n",
        "                    'bbc' not in text.lower()[:20] and\n",
        "                    'copyright' not in text.lower()):\n",
        "                    paragraphs.append(text)\n",
        "\n",
        "            body_text = \"\\n\".join(paragraphs)\n",
        "\n",
        "            # Save if substantial\n",
        "            if len(body_text) > 300 and title and len(paragraphs) >= 3:\n",
        "                metadata[str(article_id)] = {\n",
        "                    \"title\": title,\n",
        "                    \"publish_date\": publish_date\n",
        "                }\n",
        "                raw_articles[article_id] = body_text\n",
        "                article_id += 1\n",
        "            else:\n",
        "                failed_count += 1\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(f\"Successfully scraped: {len(metadata)} articles\")\n",
        "    print(f\"Failed/skipped: {failed_count} articles\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return metadata, raw_articles\n",
        "\n",
        "# Execute scraping\n",
        "print(\"Starting BBC Urdu article scraper...\")\n",
        "print(\"=\"*60)\n",
        "metadata, raw_articles = scrape_bbc_urdu_articles(target_count=300)\n",
        "\n",
        "# Save metadata JSON\n",
        "with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Saved metadata for {len(metadata)} articles to metadata.json\")\n",
        "print(f\"✓ Collected {len(raw_articles)} article bodies\")\n",
        "\n",
        "# Show sample\n",
        "if metadata:\n",
        "    print(f\"\\nSample article:\")\n",
        "    first_id = list(metadata.keys())[0]\n",
        "    print(f\"  Title: {metadata[first_id]['title']}\")\n",
        "    print(f\"  Date: {metadata[first_id]['publish_date']}\")\n",
        "    print(f\"  Body length: {len(raw_articles[int(first_id)])} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DNGhW8IhxGs"
      },
      "source": [
        "### ***raw.txt File***\n",
        "This file must contain:\n",
        "- Raw scraped article content\n",
        "- No cleaning or normalization\n",
        "- One article per block\n",
        "- Each article must start with its article number\n",
        "\n",
        "### ***Example***\n",
        "```json\n",
        "[1]\n",
        "یہ خبر بی بی سی اردو سے حاصل کی گئی ہے...\n",
        "\n",
        "[2]\n",
        "کراچی میں بارش کے بعد صورتحال خراب ہو گئی..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lue1ht7ahxGu",
        "outputId": "b2a5855e-6240-4277-aa04-d02202ab76f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 300 raw articles to raw.txt\n"
          ]
        }
      ],
      "source": [
        "# Save raw articles to raw.txt\n",
        "with open(\"raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for article_id in sorted(raw_articles.keys()):\n",
        "        f.write(f\"[{article_id}]\\n\")\n",
        "        f.write(raw_articles[article_id])\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "print(f\"Saved {len(raw_articles)} raw articles to raw.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz7xmzMEhxGw"
      },
      "source": [
        "## ***cleaned.txt File***\n",
        "\n",
        "This file must contain:\n",
        "- Fully preprocessed data\n",
        "- Normalized Urdu text\n",
        "- Noise removed\n",
        "- Sentence segmented\n",
        "- Ready for language model training\n",
        "- Article numbering matching raw.txt and JSON\n",
        "\n",
        "**Refer to the given Assignment PDF Document for the data cleaning and normalizing techniques**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6icWJJ6hxGx",
        "outputId": "f595926e-3803-49b7-8ebc-9baa68f2bae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STARTING DATA CLEANING PROCESS\n",
            "======================================================================\n",
            "\n",
            "Cleaning Article 1:\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 141 sentences\n",
            "  Original: 12358 chars → Cleaned: 11959 chars\n",
            "  Reduction: 3.2%\n",
            "\n",
            "Cleaning Article 2:\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 51 sentences\n",
            "  Original: 6591 chars → Cleaned: 6211 chars\n",
            "  Reduction: 5.8%\n",
            "\n",
            "Cleaning Article 3:\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Original: 6651 chars → Cleaned: 6281 chars\n",
            "  Reduction: 5.6%\n",
            "\n",
            "Cleaning remaining 297 articles...\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 88 sentences\n",
            "  Removed 72 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 136 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 24 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 53 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 95 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 44 sentences\n",
            "  Processed 10/300 articles...\r  Removed 29 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 157 sentences\n",
            "  Removed 25 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 65 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 47 sentences\n",
            "  Removed 33 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 54 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 24 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 121 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Removed 35 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 132 sentences\n",
            "  Processed 20/300 articles...\r  Removed 43 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 112 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 32 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 22 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 47 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 27 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 122 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 111 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 59 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 30 sentences\n",
            "  Removed 21 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 93 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 57 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 144 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 32 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 51 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 40 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 2 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 42 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 81 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 65 sentences\n",
            "  Removed 23 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 42 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 21 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 54 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 33 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 36 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 32 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 26 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 27 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 22 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 18 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 30 sentences\n",
            "  Removed 106 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 242 sentences\n",
            "  Removed 24 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 74 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 118 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 42 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 72 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 74 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 27 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 25 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 51 sentences\n",
            "  Removed 30 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 140 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 18 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 5 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 24 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 2 sentences\n",
            "  Removed 26 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 52 sentences\n",
            "  Removed 19 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 54 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 64 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 41 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 68 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 15 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 31 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 41 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 40 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 29 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 68 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 68 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 68 sentences\n",
            "  Removed 18 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 74 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 47 sentences\n",
            "  Removed 16 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 118 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 30 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 63 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 70 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 65 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 4 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 63 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 36 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 86 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 37 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 2 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 24 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 43 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 83 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 82 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 33 sentences\n",
            "  Removed 17 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 89 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 41 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 16 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 74 sentences\n",
            "  Removed 23 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 62 sentences\n",
            "  Removed 22 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 30 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 36 sentences\n",
            "  Removed 17 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 53 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 20 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 127 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 43 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 57 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 121 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 82 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 70 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 24 sentences\n",
            "  Removed 24 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 77 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 50 sentences\n",
            "  Removed 41 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 128 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 61 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 66 sentences\n",
            "  Removed 18 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 52 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 69 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 146 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 148 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 57 sentences\n",
            "  Removed 17 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 25 sentences\n",
            "  Removed 65 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 176 sentences\n",
            "  Removed 51 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 136 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 41 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removed 11 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 30 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 90 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 134 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 50 sentences\n",
            "  Removed 18 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 3 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 25 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 59 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 50 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 22 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 72 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 24 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 86 sentences\n",
            "  Removed 36 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 101 sentences\n",
            "  Removed 11 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 94 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 42 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 51 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 84 sentences\n",
            "  Removed 27 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 21 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 51 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 97 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 50 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 20 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 66 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 68 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 42 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 87 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 17 sentences\n",
            "  Removed 21 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 47 sentences\n",
            "  Removed 30 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 63 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 92 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 98 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 11 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 43 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 34 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 23 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 61 sentences\n",
            "  Removed 38 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 92 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 31 sentences\n",
            "  Removed 22 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 61 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 32 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 84 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 64 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 35 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 128 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 31 sentences\n",
            "  Removed 9 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 93 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 90 sentences\n",
            "  Removed 24 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 45 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 96 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 59 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 49 sentences\n",
            "  Removed 11 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 82 sentences\n",
            "  Removed 28 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 163 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 25 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 34 sentences\n",
            "  Removed 27 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 53 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 89 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removed 61 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 94 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 30 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 70 sentences\n",
            "  Removed 1 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 29 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 76 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 48 sentences\n",
            "  Removed 51 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 73 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 53 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 15 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 28 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 43 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 77 sentences\n",
            "  Removed 33 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 65 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 25 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 32 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 106 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 37 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 38 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 37 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 41 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 40 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 36 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 46 sentences\n",
            "  Removed 12 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 72 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 33 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 33 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 80 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 2 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 84 sentences\n",
            "  Removed 2 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 71 sentences\n",
            "  Removed 49 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 83 sentences\n",
            "  Removed 21 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 34 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 4 sentences\n",
            "  Removed 18 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 110 sentences\n",
            "  Removed 30 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 71 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 56 sentences\n",
            "  Removed 34 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 121 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 7 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 178 sentences\n",
            "  Removed 20 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 72 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 39 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 55 sentences\n",
            "  Removed 19 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 43 sentences\n",
            "  Removed 4 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 29 sentences\n",
            "  Removed 33 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 60 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 26 sentences\n",
            "  Removed 25 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 124 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 102 sentences\n",
            "  Removed 13 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 78 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 33 sentences\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 64 sentences\n",
            "  Removed 10 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 111 sentences\n",
            "  Removed 36 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 99 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 54 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 78 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 70 sentences\n",
            "  Removed 23 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 80 sentences\n",
            "  Removed 6 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 58 sentences\n",
            "  Removed 29 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 63 sentences\n",
            "  Removed 5 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 102 sentences\n",
            "  Removed 14 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 63 sentences\n",
            "  Removed 43 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 101 sentences\n",
            "  Removed 28 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 67 sentences\n",
            "  Removed 8 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 117 sentences\n",
            "  Removed 3 diacritic marks\n",
            "  Removing noise...\n",
            "  Removing non-Urdu text...\n",
            "  Segmenting sentences...\n",
            "  Created 39 sentences\n",
            "  Processed 300/300 articles...\n",
            "======================================================================\n",
            "CLEANING SUMMARY\n",
            "======================================================================\n",
            "Total articles cleaned: 300\n",
            "Total original characters: 2,134,796\n",
            "Total cleaned characters: 2,022,815\n",
            "Overall reduction: 5.2%\n",
            "\n",
            "======================================================================\n",
            "SAMPLE CLEANED TEXT (Article 1):\n",
            "======================================================================\n",
            "نوٹ اس تحریر کے کچھ حصے قارئین کے لیے پریشانی کا باعث ہو سکتے ہیں معروف امریکی ریپر اور میوزک پروڈیوسر شان جان کومبز عرف ڈڈی پس منظر میں چلتی موسیقی کے دوران ایک وائس نوٹ میں اپنے پرسنل اسسٹنٹ سے کہتے ہیں کہ کیا آپ اوپر آ کر چیزیں ٹھیک کر سکتے ہیں۔ یہ منظر پرتعیش نہیں لگ رہا۔ کچھ گھنٹے قبل یہاں وائلڈ کنگ نائٹ نامی پارٹی چل رہی تھی جس میں نشے میں دھت کئی لوگ سیکس کرتے ہیں۔ اب عملے سے کہا جا رہا تھا\n",
            "\n",
            "... (showing first 400 characters of 11959 total)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DIACRITICS REMOVAL\n",
        "# ============================================================================\n",
        "# Urdu diacritics (zabar, zer, pesh, etc.) - Complete list\n",
        "URDU_DIACRITICS = [\n",
        "    '\\u064B',  # Fathatan (double zabar)\n",
        "    '\\u064C',  # Dammatan (double pesh)\n",
        "    '\\u064D',  # Kasratan (double zer)\n",
        "    '\\u064E',  # Fatha (zabar)\n",
        "    '\\u064F',  # Damma (pesh)\n",
        "    '\\u0650',  # Kasra (zer)\n",
        "    '\\u0651',  # Shadda (tashdeed)\n",
        "    '\\u0652',  # Sukun (jazm)\n",
        "    '\\u0653',  # Maddah\n",
        "    '\\u0654',  # Hamza above\n",
        "    '\\u0655',  # Hamza below\n",
        "    '\\u0656',  # Subscript alef\n",
        "    '\\u0657',  # Inverted damma\n",
        "    '\\u0658',  # Mark noon ghunna\n",
        "    '\\u0670',  # Superscript alef\n",
        "]\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    \"\"\"\n",
        "    Remove all Urdu diacritical marks from text.\n",
        "    Diacritics are pronunciation marks that are inconsistently used.\n",
        "    Removing them ensures uniform word representation.\n",
        "\n",
        "    Example: عِلم -> علم\n",
        "    \"\"\"\n",
        "    original_length = len(text)\n",
        "\n",
        "    # Remove each diacritic\n",
        "    for diacritic in URDU_DIACRITICS:\n",
        "        text = text.replace(diacritic, '')\n",
        "\n",
        "    # Also use Unicode normalization to catch any remaining combining marks\n",
        "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
        "\n",
        "    removed = original_length - len(text)\n",
        "    if removed > 0:\n",
        "        print(f\"  Removed {removed} diacritic marks\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: NOISE REMOVAL\n",
        "# ============================================================================\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Remove all URLs and web addresses\"\"\"\n",
        "    # Remove http/https URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    # Remove www URLs\n",
        "    text = re.sub(r'www\\.\\S+', '', text)\n",
        "    # Remove domain-like patterns\n",
        "    text = re.sub(r'\\S+\\.com\\S*', '', text)\n",
        "    text = re.sub(r'\\S+\\.org\\S*', '', text)\n",
        "    text = re.sub(r'\\S+\\.net\\S*', '', text)\n",
        "    return text\n",
        "\n",
        "def remove_email_addresses(text):\n",
        "    \"\"\"Remove email addresses\"\"\"\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
        "    return text\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "    Remove special characters, symbols, and emojis.\n",
        "    Keep Urdu punctuation marks (۔ ؟ ، ٪)\n",
        "    \"\"\"\n",
        "    # Remove emojis (comprehensive range)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    # Remove special symbols but keep Urdu punctuation\n",
        "    text = re.sub(r'[#@$%^&*()_+=\\[\\]{}|\\\\<>~`\"\\']', '', text)\n",
        "\n",
        "    # Remove mathematical symbols\n",
        "    text = re.sub(r'[×÷±≠≈∞√∑∫]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_bbc_artifacts(text):\n",
        "    \"\"\"\n",
        "    Remove BBC-specific text artifacts like:\n",
        "    - Copyright notices\n",
        "    - Navigation text\n",
        "    - Image captions\n",
        "    - Social media prompts\n",
        "    \"\"\"\n",
        "    # Common BBC Urdu artifacts\n",
        "    artifacts = [\n",
        "        r'©.*?بی بی سی.*',\n",
        "        r'بی بی سی.*?ذمہ دار نہیں.*',\n",
        "        r'تصویر کا ذریعہ.*',\n",
        "        r'تصویر کا کیپشن.*',\n",
        "        r'End of.*',\n",
        "        r'سب سے زیادہ پڑھی جانے والی.*',\n",
        "        r'بی بی سی اردو کی خبروں.*?معنی رکھتی ہیں',\n",
        "        r'بیرونی لنکس کے بارے میں.*',\n",
        "    ]\n",
        "\n",
        "    for artifact in artifacts:\n",
        "        text = re.sub(artifact, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_noise(text):\n",
        "    \"\"\"Apply all noise removal steps\"\"\"\n",
        "    print(\"  Removing noise...\")\n",
        "    text = remove_urls(text)\n",
        "    text = remove_email_addresses(text)\n",
        "    text = remove_bbc_artifacts(text)\n",
        "    text = remove_special_characters(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: NON-URDU TEXT REMOVAL\n",
        "# ============================================================================\n",
        "\n",
        "def remove_english_text(text):\n",
        "    \"\"\"\n",
        "    Remove English letters and Roman Urdu.\n",
        "    Keep only Urdu script characters.\n",
        "    \"\"\"\n",
        "    # Remove English letters (both cases)\n",
        "    text = re.sub(r'[a-zA-Z]+', '', text)\n",
        "    return text\n",
        "\n",
        "def keep_only_urdu_script(text):\n",
        "    \"\"\"\n",
        "    Keep only Urdu/Arabic script characters, numbers, and Urdu punctuation.\n",
        "    Unicode ranges:\n",
        "    - 0600-06FF: Arabic (includes Urdu)\n",
        "    - 0750-077F: Arabic Supplement\n",
        "    - FB50-FDFF: Arabic Presentation Forms-A\n",
        "    - FE70-FEFF: Arabic Presentation Forms-B\n",
        "    \"\"\"\n",
        "    # Define what to keep\n",
        "    urdu_pattern = r'[\\u0600-\\u06FF\\u0750-\\u077F\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s\\d۔؟،٪!\\-]'\n",
        "\n",
        "    # Extract only matching characters\n",
        "    urdu_chars = re.findall(urdu_pattern, text)\n",
        "    return ''.join(urdu_chars)\n",
        "\n",
        "def remove_non_urdu(text):\n",
        "    \"\"\"Remove all non-Urdu text\"\"\"\n",
        "    print(\"  Removing non-Urdu text...\")\n",
        "    text = remove_english_text(text)\n",
        "    text = keep_only_urdu_script(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: SENTENCE SEGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "def segment_sentences(text):\n",
        "    \"\"\"\n",
        "    Segment text into sentences using Urdu punctuation marks.\n",
        "    Urdu sentence endings: ۔ (full stop), ؟ (question mark), ! (exclamation)\n",
        "    \"\"\"\n",
        "    print(\"  Segmenting sentences...\")\n",
        "\n",
        "    # Split on Urdu sentence terminators\n",
        "    sentences = re.split(r'[۔؟!]+', text)\n",
        "\n",
        "    # Clean each sentence\n",
        "    cleaned_sentences = []\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        # Keep sentences with at least 3 words\n",
        "        if sent and len(sent.split()) >= 3:\n",
        "            cleaned_sentences.append(sent)\n",
        "\n",
        "    # Rejoin with Urdu full stop and space\n",
        "    if cleaned_sentences:\n",
        "        result = '۔ '.join(cleaned_sentences) + '۔'\n",
        "        print(f\"  Created {len(cleaned_sentences)} sentences\")\n",
        "        return result\n",
        "    return ''\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: WHITESPACE NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    \"\"\"\n",
        "    Normalize all whitespace:\n",
        "    - Replace multiple spaces with single space\n",
        "    - Remove leading/trailing whitespace\n",
        "    - Normalize line breaks\n",
        "    \"\"\"\n",
        "    # Replace multiple spaces with single space\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # Replace multiple line breaks\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "\n",
        "    # Remove tabs\n",
        "    text = text.replace('\\t', ' ')\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    # Ensure proper spacing after Urdu punctuation\n",
        "    text = re.sub(r'([۔؟،])([^\\s])', r'\\1 \\2', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MASTER CLEANING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def clean_article(text, article_id=None):\n",
        "    \"\"\"\n",
        "    Apply all cleaning steps to an article in sequence.\n",
        "    Returns cleaned text ready for tokenization.\n",
        "    \"\"\"\n",
        "    if article_id:\n",
        "        print(f\"\\nCleaning Article {article_id}:\")\n",
        "\n",
        "    original_length = len(text)\n",
        "\n",
        "    # Step 1: Remove diacritics\n",
        "    text = remove_diacritics(text)\n",
        "\n",
        "    # Step 2: Remove noise\n",
        "    text = remove_noise(text)\n",
        "\n",
        "    # Step 3: Remove non-Urdu text\n",
        "    text = remove_non_urdu(text)\n",
        "\n",
        "    # Step 4: Normalize whitespace\n",
        "    text = normalize_whitespace(text)\n",
        "\n",
        "    # Step 5: Segment sentences\n",
        "    text = segment_sentences(text)\n",
        "\n",
        "    final_length = len(text)\n",
        "    reduction = ((original_length - final_length) / original_length * 100) if original_length > 0 else 0\n",
        "\n",
        "    if article_id:\n",
        "        print(f\"  Original: {original_length} chars → Cleaned: {final_length} chars\")\n",
        "        print(f\"  Reduction: {reduction:.1f}%\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CLEAN ALL ARTICLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING DATA CLEANING PROCESS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cleaned_articles = {}\n",
        "total_original_chars = 0\n",
        "total_cleaned_chars = 0\n",
        "\n",
        "# Show detailed cleaning for first 3 articles\n",
        "for article_id, raw_text in list(raw_articles.items())[:3]:\n",
        "    cleaned_text = clean_article(raw_text, article_id)\n",
        "    cleaned_articles[article_id] = cleaned_text\n",
        "    total_original_chars += len(raw_text)\n",
        "    total_cleaned_chars += len(cleaned_text)\n",
        "\n",
        "# Clean remaining articles without detailed output\n",
        "print(f\"\\nCleaning remaining {len(raw_articles) - 3} articles...\")\n",
        "for article_id, raw_text in list(raw_articles.items())[3:]:\n",
        "    cleaned_text = clean_article(raw_text)\n",
        "    cleaned_articles[article_id] = cleaned_text\n",
        "    total_original_chars += len(raw_text)\n",
        "    total_cleaned_chars += len(cleaned_text)\n",
        "\n",
        "    # Progress indicator\n",
        "    if article_id % 10 == 0:\n",
        "        print(f\"  Processed {article_id}/{len(raw_articles)} articles...\", end='\\r')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLEANING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total articles cleaned: {len(cleaned_articles)}\")\n",
        "print(f\"Total original characters: {total_original_chars:,}\")\n",
        "print(f\"Total cleaned characters: {total_cleaned_chars:,}\")\n",
        "print(f\"Overall reduction: {((total_original_chars - total_cleaned_chars) / total_original_chars * 100):.1f}%\")\n",
        "\n",
        "# Show sample of cleaned text\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE CLEANED TEXT (Article 1):\")\n",
        "print(\"=\"*70)\n",
        "if 1 in cleaned_articles:\n",
        "    sample = cleaned_articles[1][:400]\n",
        "    print(sample)\n",
        "    print(f\"\\n... (showing first 400 characters of {len(cleaned_articles[1])} total)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0OaxqhxG1"
      },
      "source": [
        "## ***Custom Urdu Tokenizer***\n",
        "\n",
        "The tokenizer must handle:\n",
        "- Word boundaries\n",
        "- Punctuation\n",
        "- Postpositions\n",
        "- Numbers and special tokens\n",
        "\n",
        "All numbers must be replaced with `<NUM>`.\n",
        "\n",
        "Input:\n",
        "\n",
        "پاکستان میں میں بارش ہوئی  \n",
        "2024  \n",
        "\n",
        "Output:\n",
        "\n",
        "پاکستان | میں | میں | بارش | ہوئی  \n",
        "`<NUM>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sguY6XtYhxG2",
        "outputId": "2d7dbfad-1371-48f0-dd58-65f7d920ab64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "URDU TOKENIZER\n",
            "======================================================================\n",
            "\n",
            "Test Cases:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Input:  پاکستان میں میں بارش ہوئی 2024\n",
            "   Tokens: پاکستان | میں | میں | بارش | ہوئی | <NUM>\n",
            "   Count:  6 tokens\n",
            "\n",
            "2. Input:  حکومت نے 15 فروری کو اعلان کیا۔\n",
            "   Tokens: حکومت | نے | <NUM> | فروری | کو | اعلان | کیا | ۔\n",
            "   Count:  8 tokens\n",
            "\n",
            "3. Input:  کیا آپ 100 روپے دیں گے؟\n",
            "   Tokens: کیا | آپ | <NUM> | روپے | دیں | گے | ؟\n",
            "   Count:  7 tokens\n",
            "\n",
            "======================================================================\n",
            "TOKENIZING ALL ARTICLES\n",
            "======================================================================\n",
            "\n",
            "Detailed tokenization of Article 1:\n",
            "  Total tokens: 2797\n",
            "  Unique tokens: 788\n",
            "  Numbers: 21\n",
            "  Punctuation: 175\n",
            "  Words: 2601\n",
            "\n",
            "  First 20 tokens: نوٹ | اس | تحریر | کے | کچھ | حصے | قارئین | کے | لیے | پریشانی | کا | باعث | ہو | سکتے | ہیں | معروف | امریکی | ریپر | اور | میوزک\n",
            "\n",
            "Tokenizing remaining 299 articles...\n",
            "  Processed 300/300 articles...\n",
            "======================================================================\n",
            "TOKENIZATION SUMMARY\n",
            "======================================================================\n",
            "Articles tokenized: 300\n",
            "Total tokens: 467,100\n",
            "Vocabulary size (unique tokens): 147,546\n",
            "Numbers replaced: 3,301\n",
            "Punctuation marks: 24,922\n",
            "Word tokens: 438,877\n",
            "Average tokens per article: 1557.0\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class UrduTokenizer:\n",
        "    \"\"\"\n",
        "    Custom Urdu tokenizer that handles:\n",
        "    - Word boundaries\n",
        "    - Urdu punctuation marks\n",
        "    - Postpositions (نے، کو، سے، میں، etc.)\n",
        "    - Numbers (replaced with <NUM> token)\n",
        "    - Special tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Urdu punctuation marks\n",
        "        self.urdu_punctuation = '۔؟،٪!'\n",
        "\n",
        "        # Common Urdu postpositions (should be kept as separate tokens)\n",
        "        self.postpositions = [\n",
        "            'نے', 'کو', 'سے', 'میں', 'پر', 'کا', 'کی', 'کے',\n",
        "            'تک', 'کیلیے', 'کیلئے', 'بھی', 'ہی', 'تو', 'ہے'\n",
        "        ]\n",
        "\n",
        "        # Special tokens\n",
        "        self.special_tokens = {\n",
        "            '<NUM>': 'number',\n",
        "            '<START>': 'sentence start',\n",
        "            '<END>': 'sentence end',\n",
        "        }\n",
        "\n",
        "    def replace_numbers(self, text):\n",
        "        \"\"\"\n",
        "        Replace all numbers (both English and Urdu digits) with <NUM> token.\n",
        "\n",
        "        Examples:\n",
        "        - 2024 -> <NUM>\n",
        "        - ۲۰۲۴ -> <NUM>\n",
        "        - 12.5 -> <NUM>\n",
        "        \"\"\"\n",
        "        # Replace English digits\n",
        "        text = re.sub(r'\\d+\\.?\\d*', '<NUM>', text)\n",
        "\n",
        "        # Replace Urdu/Arabic digits (۰-۹)\n",
        "        text = re.sub(r'[۰-۹]+', '<NUM>', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize Urdu text into words and punctuation.\n",
        "\n",
        "        Process:\n",
        "        1. Replace numbers with <NUM>\n",
        "        2. Separate punctuation from words\n",
        "        3. Split on whitespace\n",
        "        4. Filter empty tokens\n",
        "\n",
        "        Returns: List of tokens\n",
        "        \"\"\"\n",
        "        # Step 1: Replace numbers\n",
        "        text = self.replace_numbers(text)\n",
        "\n",
        "        # Step 2: Add spaces around punctuation for easier splitting\n",
        "        for punct in self.urdu_punctuation:\n",
        "            text = text.replace(punct, f' {punct} ')\n",
        "\n",
        "        # Step 3: Split on whitespace\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Step 4: Clean tokens\n",
        "        final_tokens = []\n",
        "        for token in tokens:\n",
        "            token = token.strip()\n",
        "            if token:  # Only add non-empty tokens\n",
        "                final_tokens.append(token)\n",
        "\n",
        "        return final_tokens\n",
        "\n",
        "    def tokenize_with_stats(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize and return statistics.\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "\n",
        "        stats = {\n",
        "            'total_tokens': len(tokens),\n",
        "            'unique_tokens': len(set(tokens)),\n",
        "            'num_count': tokens.count('<NUM>'),\n",
        "            'punct_count': sum(1 for t in tokens if t in self.urdu_punctuation),\n",
        "            'word_count': sum(1 for t in tokens if t not in self.urdu_punctuation and t != '<NUM>')\n",
        "        }\n",
        "\n",
        "        return tokens, stats\n",
        "\n",
        "    def detokenize(self, tokens):\n",
        "        \"\"\"\n",
        "        Convert tokens back to text.\n",
        "        Handles proper spacing around punctuation.\n",
        "        \"\"\"\n",
        "        if not tokens:\n",
        "            return ''\n",
        "\n",
        "        result = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token in self.urdu_punctuation:\n",
        "                # Punctuation: no space before, space after (except at end)\n",
        "                if result:\n",
        "                    result[-1] += token\n",
        "                else:\n",
        "                    result.append(token)\n",
        "                if i < len(tokens) - 1:\n",
        "                    result.append('')  # Will add space\n",
        "            else:\n",
        "                result.append(token)\n",
        "\n",
        "        return ' '.join(result).replace('  ', ' ').strip()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# INITIALIZE AND TEST TOKENIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"URDU TOKENIZER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenizer = UrduTokenizer()\n",
        "\n",
        "# Test with examples\n",
        "test_cases = [\n",
        "    \"پاکستان میں میں بارش ہوئی 2024\",\n",
        "    \"حکومت نے 15 فروری کو اعلان کیا۔\",\n",
        "    \"کیا آپ 100 روپے دیں گے؟\"\n",
        "]\n",
        "\n",
        "print(\"\\nTest Cases:\")\n",
        "print(\"-\" * 70)\n",
        "for i, test_text in enumerate(test_cases, 1):\n",
        "    tokens = tokenizer.tokenize(test_text)\n",
        "    print(f\"\\n{i}. Input:  {test_text}\")\n",
        "    print(f\"   Tokens: {' | '.join(tokens)}\")\n",
        "    print(f\"   Count:  {len(tokens)} tokens\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOKENIZE ALL CLEANED ARTICLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOKENIZING ALL ARTICLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenized_articles = {}\n",
        "all_stats = {\n",
        "    'total_tokens': 0,\n",
        "    'total_unique': 0,\n",
        "    'total_numbers': 0,\n",
        "    'total_punct': 0,\n",
        "    'total_words': 0\n",
        "}\n",
        "\n",
        "# Tokenize first article with details\n",
        "if 1 in cleaned_articles:\n",
        "    print(\"\\nDetailed tokenization of Article 1:\")\n",
        "    tokens, stats = tokenizer.tokenize_with_stats(cleaned_articles[1])\n",
        "    tokenized_articles[1] = tokens\n",
        "\n",
        "    print(f\"  Total tokens: {stats['total_tokens']}\")\n",
        "    print(f\"  Unique tokens: {stats['unique_tokens']}\")\n",
        "    print(f\"  Numbers: {stats['num_count']}\")\n",
        "    print(f\"  Punctuation: {stats['punct_count']}\")\n",
        "    print(f\"  Words: {stats['word_count']}\")\n",
        "    print(f\"\\n  First 20 tokens: {' | '.join(tokens[:20])}\")\n",
        "\n",
        "    # Update cumulative stats\n",
        "    all_stats['total_tokens'] += stats['total_tokens']\n",
        "    all_stats['total_unique'] += stats['unique_tokens']\n",
        "    all_stats['total_numbers'] += stats['num_count']\n",
        "    all_stats['total_punct'] += stats['punct_count']\n",
        "    all_stats['total_words'] += stats['word_count']\n",
        "\n",
        "# Tokenize remaining articles\n",
        "print(f\"\\nTokenizing remaining {len(cleaned_articles) - 1} articles...\")\n",
        "for article_id, text in cleaned_articles.items():\n",
        "    if article_id == 1:\n",
        "        continue\n",
        "\n",
        "    tokens, stats = tokenizer.tokenize_with_stats(text)\n",
        "    tokenized_articles[article_id] = tokens\n",
        "\n",
        "    # Update cumulative stats\n",
        "    all_stats['total_tokens'] += stats['total_tokens']\n",
        "    all_stats['total_unique'] += stats['unique_tokens']\n",
        "    all_stats['total_numbers'] += stats['num_count']\n",
        "    all_stats['total_punct'] += stats['punct_count']\n",
        "    all_stats['total_words'] += stats['word_count']\n",
        "\n",
        "    if article_id % 10 == 0:\n",
        "        print(f\"  Processed {article_id}/{len(cleaned_articles)} articles...\", end='\\r')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOKENIZATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Articles tokenized: {len(tokenized_articles)}\")\n",
        "print(f\"Total tokens: {all_stats['total_tokens']:,}\")\n",
        "print(f\"Vocabulary size (unique tokens): {all_stats['total_unique']:,}\")\n",
        "print(f\"Numbers replaced: {all_stats['total_numbers']:,}\")\n",
        "print(f\"Punctuation marks: {all_stats['total_punct']:,}\")\n",
        "print(f\"Word tokens: {all_stats['total_words']:,}\")\n",
        "print(f\"Average tokens per article: {all_stats['total_tokens'] / len(tokenized_articles):.1f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEiE8KXPhxG4"
      },
      "source": [
        "## ***Custom Urdu Stemmer***\n",
        "The stemmer must remove common Urdu suffixes.\n",
        "\n",
        "Input:\n",
        "لڑکیوں نے کتابیں پڑھیں  \n",
        "\n",
        "Output:\n",
        "لڑکی | کتاب | پڑھ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA7JOioIhxG7",
        "outputId": "eba15959-80a6-47e6-d7f5-135fe0516dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer Test:\n",
            "لڑکیوں -> لڑک\n",
            "کتابیں -> کتاب\n",
            "پڑھیں -> پڑھ\n",
            "\n",
            "Stemmed 300 articles\n"
          ]
        }
      ],
      "source": [
        "class UrduStemmer:\n",
        "    \"\"\"Custom Urdu stemmer to remove common suffixes\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Common Urdu suffixes (ordered by length, longest first)\n",
        "        self.suffixes = [\n",
        "            'یوں',  # plural/oblique\n",
        "            'وں',   # plural\n",
        "            'یں',   # plural feminine\n",
        "            'ئیں',  # plural feminine\n",
        "            'یاں',  # plural feminine\n",
        "            'ئے',   # oblique\n",
        "            'ئی',   # feminine\n",
        "            'یں',   # plural\n",
        "            'ئے',   # plural\n",
        "            'وں',   # plural\n",
        "            'ون',   # plural\n",
        "            'ات',   # plural\n",
        "            'ہے',   # verb ending\n",
        "            'تھا',  # past tense\n",
        "            'تھی',  # past tense feminine\n",
        "            'تھے',  # past tense plural\n",
        "            'گا',   # future\n",
        "            'گی',   # future feminine\n",
        "            'گے',   # future plural\n",
        "            'ئی',   # feminine\n",
        "            'یا',   # past\n",
        "            'ئے',   # plural\n",
        "            'ے',    # common ending\n",
        "            'ی',    # feminine\n",
        "            'ا',    # masculine\n",
        "            'و',    # plural marker\n",
        "        ]\n",
        "\n",
        "    def stem(self, word):\n",
        "        \"\"\"Remove suffix from a word\"\"\"\n",
        "        if len(word) <= 2:  # Don't stem very short words\n",
        "            return word\n",
        "\n",
        "        # Try to remove suffixes\n",
        "        for suffix in self.suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "                return word[:-len(suffix)]\n",
        "\n",
        "        return word\n",
        "\n",
        "    def stem_tokens(self, tokens):\n",
        "        \"\"\"Stem a list of tokens\"\"\"\n",
        "        return [self.stem(token) for token in tokens]\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = UrduStemmer()\n",
        "\n",
        "# Test stemmer\n",
        "test_words = ['لڑکیوں', 'کتابیں', 'پڑھیں']\n",
        "print(\"Stemmer Test:\")\n",
        "for word in test_words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")\n",
        "\n",
        "# Stem all tokenized articles\n",
        "stemmed_articles = {}\n",
        "for article_id, tokens in tokenized_articles.items():\n",
        "    stemmed_articles[article_id] = stemmer.stem_tokens(tokens)\n",
        "\n",
        "print(f\"\\nStemmed {len(stemmed_articles)} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oymNhXVjhxG8"
      },
      "source": [
        "## ***Custom Urdu Lemmatizer***\n",
        "\n",
        "The lemmatizer is restricted to:\n",
        "- Plural normalization\n",
        "- Gender normalization\n",
        "\n",
        "Plural:\n",
        "- لڑکیاں -> لڑکی  \n",
        "- کتابوں -> کتاب  \n",
        "\n",
        "Gender:\n",
        "- اچھی -> اچھا  \n",
        "- بڑی -> بڑا"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgvzDV6rhxG9",
        "outputId": "e648327a-71ca-4cfd-97d2-7953b6f999fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatizer Test:\n",
            "لڑکیاں -> لڑکی\n",
            "کتابوں -> کتاب\n",
            "اچھی -> اچھا\n",
            "بڑی -> بڑا\n",
            "\n",
            "Lemmatized 300 articles\n"
          ]
        }
      ],
      "source": [
        "class UrduLemmatizer:\n",
        "    \"\"\"Custom Urdu lemmatizer for plural and gender normalization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Plural to singular mappings\n",
        "        self.plural_map = {\n",
        "            'لڑکیاں': 'لڑکی',\n",
        "            'لڑکیوں': 'لڑکی',\n",
        "            'کتابیں': 'کتاب',\n",
        "            'کتابوں': 'کتاب',\n",
        "            'لوگوں': 'لوگ',\n",
        "            'بچوں': 'بچہ',\n",
        "            'عورتوں': 'عورت',\n",
        "            'مردوں': 'مرد',\n",
        "        }\n",
        "\n",
        "        # Feminine to masculine mappings\n",
        "        self.gender_map = {\n",
        "            'اچھی': 'اچھا',\n",
        "            'بڑی': 'بڑا',\n",
        "            'چھوٹی': 'چھوٹا',\n",
        "            'نئی': 'نیا',\n",
        "            'پرانی': 'پرانا',\n",
        "            'خوبصورتی': 'خوبصورت',\n",
        "        }\n",
        "\n",
        "        # Plural suffixes for pattern-based lemmatization\n",
        "        self.plural_suffixes = [\n",
        "            ('یاں', 'ی'),   # لڑکیاں -> لڑکی\n",
        "            ('یوں', 'ی'),   # لڑکیوں -> لڑکی\n",
        "            ('وں', ''),     # کتابوں -> کتاب\n",
        "            ('یں', ''),     # کتابیں -> کتاب\n",
        "            ('ئیں', 'ی'),   # plural feminine\n",
        "        ]\n",
        "\n",
        "        # Gender suffixes\n",
        "        self.gender_suffixes = [\n",
        "            ('ی', 'ا'),     # feminine to masculine\n",
        "        ]\n",
        "\n",
        "    def lemmatize(self, word):\n",
        "        \"\"\"Lemmatize a single word\"\"\"\n",
        "        # Check direct mappings first\n",
        "        if word in self.plural_map:\n",
        "            return self.plural_map[word]\n",
        "        if word in self.gender_map:\n",
        "            return self.gender_map[word]\n",
        "\n",
        "        # Try pattern-based plural normalization\n",
        "        for suffix, replacement in self.plural_suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "                return word[:-len(suffix)] + replacement\n",
        "\n",
        "        # Try gender normalization (be conservative)\n",
        "        if len(word) > 3:\n",
        "            for suffix, replacement in self.gender_suffixes:\n",
        "                if word.endswith(suffix):\n",
        "                    # Only apply if it looks like an adjective\n",
        "                    potential = word[:-len(suffix)] + replacement\n",
        "                    return potential\n",
        "\n",
        "        return word\n",
        "\n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Lemmatize a list of tokens\"\"\"\n",
        "        return [self.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = UrduLemmatizer()\n",
        "\n",
        "# Test lemmatizer\n",
        "test_words = ['لڑکیاں', 'کتابوں', 'اچھی', 'بڑی']\n",
        "print(\"Lemmatizer Test:\")\n",
        "for word in test_words:\n",
        "    print(f\"{word} -> {lemmatizer.lemmatize(word)}\")\n",
        "\n",
        "# Lemmatize all tokenized articles\n",
        "lemmatized_articles = {}\n",
        "for article_id, tokens in tokenized_articles.items():\n",
        "    lemmatized_articles[article_id] = lemmatizer.lemmatize_tokens(tokens)\n",
        "\n",
        "print(f\"\\nLemmatized {len(lemmatized_articles)} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK714ikAhxG-"
      },
      "source": [
        "### ***Mandatory Deliverables for Part 1***\n",
        "- JSON metadata file\n",
        "- raw.txt\n",
        "- cleaned.txt\n",
        "- Tokenized dataset\n",
        "- Stemmed dataset\n",
        "- Lemmatized dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEUUiABbhxG_",
        "outputId": "a44cd1be-3bc6-41e6-ea76-2361686723d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved all deliverables:\n",
            "  - metadata.json\n",
            "  - raw.txt\n",
            "  - cleaned.txt\n",
            "  - tokenized.txt\n",
            "  - stemmed.txt\n",
            "  - lemmatized.txt\n",
            "\n",
            "Total articles processed: 300\n"
          ]
        }
      ],
      "source": [
        "# Save cleaned articles to cleaned.txt\n",
        "with open(\"cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for article_id in sorted(cleaned_articles.keys()):\n",
        "        f.write(f\"[{article_id}]\\n\")\n",
        "        f.write(cleaned_articles[article_id])\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "# Save tokenized dataset\n",
        "with open(\"tokenized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for article_id in sorted(tokenized_articles.keys()):\n",
        "        f.write(f\"[{article_id}]\\n\")\n",
        "        f.write(' '.join(tokenized_articles[article_id]))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "# Save stemmed dataset\n",
        "with open(\"stemmed.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for article_id in sorted(stemmed_articles.keys()):\n",
        "        f.write(f\"[{article_id}]\\n\")\n",
        "        f.write(' '.join(stemmed_articles[article_id]))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "# Save lemmatized dataset\n",
        "with open(\"lemmatized.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for article_id in sorted(lemmatized_articles.keys()):\n",
        "        f.write(f\"[{article_id}]\\n\")\n",
        "        f.write(' '.join(lemmatized_articles[article_id]))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "print(\"✓ Saved all deliverables:\")\n",
        "print(\"  - metadata.json\")\n",
        "print(\"  - raw.txt\")\n",
        "print(\"  - cleaned.txt\")\n",
        "print(\"  - tokenized.txt\")\n",
        "print(\"  - stemmed.txt\")\n",
        "print(\"  - lemmatized.txt\")\n",
        "print(f\"\\nTotal articles processed: {len(metadata)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEf8fNwEhxHB"
      },
      "source": [
        "## **Part 2: BBC Style Urdu News Article Generation**\n",
        "\n",
        "Students must implement the following statistical language models:\n",
        "- Unigram Language Model (for fallback and evaluation purposes)\n",
        "- Bigram Language Model (for fallback and evaluation purposes)\n",
        "- Trigram Language Model with fallback (Trigram → Bigram → Unigram)\n",
        "\n",
        "The article generation system must use a fallback mechanism such that when a higher-order n-gram is unavailable, the model automatically backs off to a lower-order model.\n",
        "\n",
        "Only Laplace (Add-One) smoothing is allowed for probability estimation.  \n",
        "No other smoothing methods may be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyjL6-wrhxHC",
        "outputId": "9c8a2e90-5ea5-458b-d9ac-95b8b3d2c9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens for training: 467100\n",
            "Vocabulary size: 16871\n",
            "\n",
            "Training Unigram model...\n",
            "Training Bigram model...\n",
            "Training Trigram model with backoff...\n",
            "\n",
            "✓ All models trained successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "class UnigramModel:\n",
        "    \"\"\"Unigram language model with Laplace smoothing\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=1.0):\n",
        "        self.smoothing = smoothing\n",
        "        self.word_counts = Counter()\n",
        "        self.total_words = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def train(self, tokens):\n",
        "        \"\"\"Train on a list of tokens\"\"\"\n",
        "        self.word_counts = Counter(tokens)\n",
        "        self.total_words = len(tokens)\n",
        "        self.vocab_size = len(self.word_counts)\n",
        "\n",
        "    def probability(self, word):\n",
        "        \"\"\"Calculate probability of a word with Laplace smoothing\"\"\"\n",
        "        count = self.word_counts.get(word, 0)\n",
        "        return (count + self.smoothing) / (self.total_words + self.smoothing * self.vocab_size)\n",
        "\n",
        "    def generate_word(self):\n",
        "        \"\"\"Generate a random word based on unigram probabilities\"\"\"\n",
        "        words = list(self.word_counts.keys())\n",
        "        probs = [self.probability(w) for w in words]\n",
        "        return random.choices(words, weights=probs, k=1)[0]\n",
        "\n",
        "\n",
        "class BigramModel:\n",
        "    \"\"\"Bigram language model with Laplace smoothing\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=1.0):\n",
        "        self.smoothing = smoothing\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.vocab = set()\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def train(self, tokens):\n",
        "        \"\"\"Train on a list of tokens\"\"\"\n",
        "        # Add start and end tokens\n",
        "        tokens = ['<START>'] + tokens + ['<END>']\n",
        "\n",
        "        # Count bigrams and unigrams\n",
        "        for i in range(len(tokens) - 1):\n",
        "            w1, w2 = tokens[i], tokens[i+1]\n",
        "            self.bigram_counts[w1][w2] += 1\n",
        "            self.unigram_counts[w1] += 1\n",
        "            self.vocab.add(w1)\n",
        "            self.vocab.add(w2)\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def probability(self, w1, w2):\n",
        "        \"\"\"Calculate P(w2|w1) with Laplace smoothing\"\"\"\n",
        "        bigram_count = self.bigram_counts[w1].get(w2, 0)\n",
        "        unigram_count = self.unigram_counts.get(w1, 0)\n",
        "        return (bigram_count + self.smoothing) / (unigram_count + self.smoothing * self.vocab_size)\n",
        "\n",
        "    def generate_word(self, prev_word):\n",
        "        \"\"\"Generate next word given previous word\"\"\"\n",
        "        if prev_word not in self.bigram_counts:\n",
        "            return None\n",
        "\n",
        "        possible_words = list(self.vocab)\n",
        "        probs = [self.probability(prev_word, w) for w in possible_words]\n",
        "        return random.choices(possible_words, weights=probs, k=1)[0]\n",
        "\n",
        "\n",
        "class TrigramModel:\n",
        "    \"\"\"Trigram language model with backoff to bigram and unigram\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=1.0):\n",
        "        self.smoothing = smoothing\n",
        "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.vocab = set()\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        # For backoff\n",
        "        self.bigram_model = None\n",
        "        self.unigram_model = None\n",
        "\n",
        "    def train(self, tokens):\n",
        "        \"\"\"Train on a list of tokens\"\"\"\n",
        "        # Add start and end tokens\n",
        "        tokens = ['<START>', '<START>'] + tokens + ['<END>']\n",
        "\n",
        "        # Count trigrams, bigrams, and unigrams\n",
        "        for i in range(len(tokens) - 2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "            self.trigram_counts[w1][w2][w3] += 1\n",
        "            self.bigram_counts[w2][w3] += 1\n",
        "            self.unigram_counts[w3] += 1\n",
        "            self.vocab.add(w1)\n",
        "            self.vocab.add(w2)\n",
        "            self.vocab.add(w3)\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        # Train backoff models\n",
        "        self.bigram_model = BigramModel(self.smoothing)\n",
        "        self.bigram_model.train(tokens[1:])  # Remove one START token\n",
        "\n",
        "        self.unigram_model = UnigramModel(self.smoothing)\n",
        "        self.unigram_model.train(tokens[2:])  # Remove START tokens\n",
        "\n",
        "    def probability(self, w1, w2, w3):\n",
        "        \"\"\"\n",
        "        Calculate P(w3|w1,w2) with Laplace smoothing.\n",
        "\n",
        "        Formula: P(w3|w1,w2) = (Count(w1,w2,w3) + α) / (Count(w1,w2,*) + α * V)\n",
        "        where Count(w1,w2,*) is the count of all trigrams starting with (w1,w2)\n",
        "        \"\"\"\n",
        "        trigram_count = self.trigram_counts[w1][w2].get(w3, 0)\n",
        "\n",
        "        # Count of all trigrams with context (w1, w2)\n",
        "        if w1 in self.trigram_counts and w2 in self.trigram_counts[w1]:\n",
        "            context_count = sum(self.trigram_counts[w1][w2].values())\n",
        "        else:\n",
        "            context_count = 0\n",
        "\n",
        "        # Apply Laplace smoothing\n",
        "        return (trigram_count + self.smoothing) / (context_count + self.smoothing * self.vocab_size)\n",
        "\n",
        "    def generate_word(self, w1, w2):\n",
        "        \"\"\"Generate next word with backoff: trigram -> bigram -> unigram\"\"\"\n",
        "        # Try trigram\n",
        "        if w1 in self.trigram_counts and w2 in self.trigram_counts[w1]:\n",
        "            possible_words = list(self.vocab)\n",
        "            probs = [self.probability(w1, w2, w) for w in possible_words]\n",
        "            if sum(probs) > 0:\n",
        "                return random.choices(possible_words, weights=probs, k=1)[0]\n",
        "\n",
        "        # Backoff to bigram\n",
        "        if self.bigram_model and w2 in self.bigram_model.bigram_counts:\n",
        "            return self.bigram_model.generate_word(w2)\n",
        "\n",
        "        # Backoff to unigram\n",
        "        if self.unigram_model:\n",
        "            return self.unigram_model.generate_word()\n",
        "\n",
        "        return '<END>'\n",
        "\n",
        "\n",
        "# Prepare training data from lemmatized articles\n",
        "all_tokens = []\n",
        "for article_id in sorted(lemmatized_articles.keys()):\n",
        "    all_tokens.extend(lemmatized_articles[article_id])\n",
        "\n",
        "print(f\"Total tokens for training: {len(all_tokens)}\")\n",
        "print(f\"Vocabulary size: {len(set(all_tokens))}\")\n",
        "\n",
        "# Train models\n",
        "print(\"\\nTraining Unigram model...\")\n",
        "unigram_model = UnigramModel(smoothing=0.1)\n",
        "unigram_model.train(all_tokens)\n",
        "\n",
        "print(\"Training Bigram model...\")\n",
        "bigram_model = BigramModel(smoothing=0.1)\n",
        "bigram_model.train(all_tokens)\n",
        "\n",
        "print(\"Training Trigram model with backoff...\")\n",
        "trigram_model = TrigramModel(smoothing=0.1)\n",
        "trigram_model.train(all_tokens)\n",
        "\n",
        "print(\"\\n✓ All models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeENLwSOhxHE"
      },
      "source": [
        "## ***Seed Prompt and Generation Constraints***\n",
        "\n",
        "- Input must contain 5–8 Urdu words\n",
        "- Single-word prompts are not allowed\n",
        "\n",
        "Valid:\n",
        "پاکستان میں مہنگائی کی شرح میں  \n",
        "\n",
        "Invalid:\n",
        "پاکستان  \n",
        "\n",
        "### ***Article Length Rules***\n",
        "- Minimum 150 words\n",
        "- Target up to 200 words\n",
        "- Minimum 5 sentences\n",
        "- Hard stop at 300 words\n",
        "- Forced termination if EOS is not produced\n",
        "\n",
        "### ***Article Output Requirements***\n",
        "Students must generate:\n",
        "- Three complete Urdu news articles\n",
        "- Five Urdu news headlines\n",
        "\n",
        "Generated content must not copy full sentences from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7JuoQUbhxHF",
        "outputId": "b7968557-5476-40f3-c7f4-878794775a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Article generators initialized\n",
            "\n",
            "Ready to generate articles!\n"
          ]
        }
      ],
      "source": [
        "class ArticleGenerator:\n",
        "    \"\"\"Generate Urdu news articles using trained language models\"\"\"\n",
        "\n",
        "    def __init__(self, model, model_type='bigram'):\n",
        "        self.model = model\n",
        "        self.model_type = model_type\n",
        "        self.urdu_stops = ['۔', '؟', '!']\n",
        "\n",
        "    def validate_seed(self, seed_text):\n",
        "        \"\"\"Validate seed prompt has 5-8 words\"\"\"\n",
        "        words = seed_text.strip().split()\n",
        "        if len(words) < 5 or len(words) > 8:\n",
        "            raise ValueError(f\"Seed must contain 5-8 words, got {len(words)}\")\n",
        "        return words\n",
        "\n",
        "    def generate_article(self, seed_text, min_words=150, target_words=200, max_words=300, min_sentences=5):\n",
        "        \"\"\"Generate a complete article from seed text\"\"\"\n",
        "        seed_words = self.validate_seed(seed_text)\n",
        "\n",
        "        generated = seed_words.copy()\n",
        "        sentence_count = 0\n",
        "        words_since_period = len(seed_words)\n",
        "\n",
        "        while len(generated) < max_words:\n",
        "            # Generate next word based on model type\n",
        "            if self.model_type == 'bigram':\n",
        "                prev_word = generated[-1]\n",
        "                next_word = self.model.generate_word(prev_word)\n",
        "\n",
        "            elif self.model_type == 'trigram':\n",
        "                if len(generated) >= 2:\n",
        "                    w1, w2 = generated[-2], generated[-1]\n",
        "                    next_word = self.model.generate_word(w1, w2)\n",
        "                else:\n",
        "                    next_word = self.model.unigram_model.generate_word()\n",
        "\n",
        "            else:\n",
        "                next_word = self.model.generate_word()\n",
        "\n",
        "            # Check for end conditions\n",
        "            if next_word == '<END>' or next_word is None:\n",
        "                if len(generated) >= min_words and sentence_count >= min_sentences:\n",
        "                    break\n",
        "                else:\n",
        "                    # Continue generating\n",
        "                    next_word = self.model.unigram_model.generate_word() if hasattr(self.model, 'unigram_model') else random.choice(list(self.model.vocab))\n",
        "\n",
        "            if next_word in ['<START>', '<END>']:\n",
        "                continue\n",
        "\n",
        "            generated.append(next_word)\n",
        "            words_since_period += 1\n",
        "\n",
        "            # Add sentence boundaries\n",
        "            if words_since_period >= 10 and random.random() < 0.15:\n",
        "                generated.append('۔')\n",
        "                sentence_count += 1\n",
        "                words_since_period = 0\n",
        "\n",
        "            # Force termination at max_words\n",
        "            if len(generated) >= max_words:\n",
        "                if generated[-1] not in self.urdu_stops:\n",
        "                    generated.append('۔')\n",
        "                break\n",
        "\n",
        "        # Ensure minimum sentences\n",
        "        if sentence_count < min_sentences and generated[-1] not in self.urdu_stops:\n",
        "            generated.append('۔')\n",
        "\n",
        "        return ' '.join(generated)\n",
        "\n",
        "    def generate_headline(self, seed_text=None, max_words=15):\n",
        "        \"\"\"Generate a short headline\"\"\"\n",
        "        if seed_text:\n",
        "            words = seed_text.strip().split()[:3]\n",
        "        else:\n",
        "            # Start with random words\n",
        "            words = [self.model.unigram_model.generate_word() if hasattr(self.model, 'unigram_model') else random.choice(list(self.model.vocab))]\n",
        "\n",
        "        generated = words.copy()\n",
        "\n",
        "        while len(generated) < max_words:\n",
        "            if self.model_type == 'bigram':\n",
        "                next_word = self.model.generate_word(generated[-1])\n",
        "            elif self.model_type == 'trigram' and len(generated) >= 2:\n",
        "                next_word = self.model.generate_word(generated[-2], generated[-1])\n",
        "            else:\n",
        "                next_word = self.model.unigram_model.generate_word() if hasattr(self.model, 'unigram_model') else random.choice(list(self.model.vocab))\n",
        "\n",
        "            if next_word and next_word not in ['<START>', '<END>', '۔', '؟']:\n",
        "                generated.append(next_word)\n",
        "\n",
        "            # Stop at reasonable headline length\n",
        "            if len(generated) >= 8 and random.random() < 0.3:\n",
        "                break\n",
        "\n",
        "        return ' '.join(generated)\n",
        "\n",
        "\n",
        "# Create generators\n",
        "bigram_generator = ArticleGenerator(bigram_model, 'bigram')\n",
        "trigram_generator = ArticleGenerator(trigram_model, 'trigram')\n",
        "\n",
        "print(\"✓ Article generators initialized\")\n",
        "print(\"\\nReady to generate articles!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uy3UPxNhxHF"
      },
      "source": [
        "### ***UI Requirements (Bonus)***\n",
        "The system must allow:\n",
        "- Model selection (Bigram / Trigram)\n",
        "- Seed input\n",
        "- Article generation\n",
        "- Proper Right-to-Left Urdu display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5cl7eVkhxHG",
        "outputId": "51bfedd8-8798-446e-9499-8b4fe5279d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BIGRAM MODEL - Generated Articles\n",
            "============================================================\n",
            "\n",
            "--- Article 1 ---\n",
            "Seed: پاکستان میں حالات کے بارے میں\n",
            "\n",
            "پاکستان میں حالات کے بارے میں افغانستان ٹورز کہے مبہم خریدے معلوماتا فرحت ڈائریکشن مشروب لپٹا گرفت بٹالوا بومبے الرمات بہہ پیاسے نانک حزب جتاتا بیریز گیلری تکنیک فران جارجینا اوزوریس دیویندر ڈسٹربیوٹرز اختراع ادھا گنگنا دھرنے ۔ میں ایک ڈانٹنے اجتناب سیریئس پیریڈز ایجر ملٹرا صندوق سبوتاژ ٹلبرگ ناہموار ۔ انھ پہن تبلو فارمیشن سائکیا دپیکا سوراخ تیسرے کیتھولکس انفارمیشن نہج ۔ ایک پائلٹ گوگرا کلیر دھوتے پابندا البیلا مقام رنجیت علامہ ۔ شیپرڈ ہلز اعتراض لڑائا انڈیگو لازم لفٹ کودکھایا کارروائا اینڈنگ ۔ دفاعا پھلے لارن چرچا ہے ۔ بخار سیلیبرٹیز اینگرا اسٹریٹ ۔ گریگورا نشاط ایڈیشن اونچ مرتد ہو عتاب ہائپر نعمت فلٹر ۔ ، اسی بے پارٹی گرفتارا شرم سمجھاؤ طنزا ریذیڈنسیز بیلیو ۔ خاص گالف سلفیٹ آثار صارف گن پیرنٹنگ کر گانے کی ۔ بھیک خالد مانتے فاسو ریڈر بھیڑی خرم کیبل لیک شکایت آبزرویٹرا فرورا کھلونے ۔ کسی افسر اڈولیٹرا رجسٹرار دنیا عنخ لتھوینیا سٹیفن سنبھال ایکسوکیلیٹن ۔ جعفر سونینا سیکھا ارب ڈالر کاٹنا سٹنرز ہالووین انکا دہرائا صاحبزادے لہرا برسر الباطن حفاظتا باصلاحیت نیرجا میڈ خواہشمند سنان کوہلا نجیب مارٹم اوبر ۔ فیلنگ عدیل لینے پاؤل کھیلنے امپریل بیونس شراب امیجنگ گلابا ملک قلم پورنوگرافا وائلیتا تبادلہ اپناتا جذباتا ٹکڑ رپورٹںگ ۔ جنازے مکھا ادتیا انسانا غلام امبالہ ٹرپل باوقار بشال پینے میز کہمنفا ۔ اس کے کام کروا سمجھو شیلبا شوچا موم جاگ ٹھیس تختہ ۔ یاد قرنیہ ملینیم وسنت اللہ صعوبت بینجمن نوٹیفکیشن رسٹ ایکسرے ناطے جنگا ۔ ہدایتکارہ چمپینزی ملاٹھے باگے ڈیورنڈ خامی جونیئر پٹڑا نہانا خضر سیزنز اجتماع اور<NUM> ملازمہ ورڈز لیونڈ مرمتا امینا ایماندار سخوئا چھچھوندر مہاراشٹر روکو آسام فیاض چاچا سرگوشی بسے ۔ لیکن ٹاؤن نٹالا شہادت جوڈیشیل غالبا پتنا جاگا رائل شور پلک اپرادھ تورانا گوپا ۔ ان ایز نامے ورثے گولڑہ طلباء سپلیمنٹ سیکٹر سولے ادارت فریز جاز پرسکون چاہیے ریکورا بنوایا ۔ بعد کینٹلا ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "--- Article 2 ---\n",
            "Seed: حکومت نے اعلان کیا ہے کہ\n",
            "\n",
            "حکومت نے اعلان کیا ہے کہ قانونا مالابار جتھا العسکر ۔ اس دوران چیختے ہندوکش ٹھکان کولاج صفدر ڈکٹیٹ گوٹ گوئنگ ہاروے ۔ کنسرٹس ٹیسٹ بھوپال متصادم اسلاما ماں پلمونرا دیویکا انجانا بمبار اسسٹنٹ لینٹیکولر بچھانا سمجھوتا امتیازا سٹیڈیم ۔ یہ قلابازی نافرمانا فضلے رضیہ وژن شناسا پھلی چلی ڈرمٹ مصنفہ یوں رفت مشی میدان زیرعتاب آکسیڈائز موناکو ڈرائنگز جیولر مذاق سیاستدان پیار سےدکھائا کیسرا ویبو ایرن دیں اچھا تیکھا رجہان پیکج حشرات بتانا ماریانہ فرزند ۔ ان کا تعلق جگہ اثرانداز طلعت ناامید ترجمانا میکر ایٹا بدتمیز ۔ یادو میگا فریقین حص یرغمالا تحریک کو انرٹیشنل میریلن پٹیل مشرا کریٹیے میکانک ۔ پولیس ستیہ کلومیٹر چھاپہ باہر رولز ٹاکیز ادارے کو یہاں ۔ دیوار آائیڈل پٹودا دشمنا پرنسلا کلونت حیدر تحاشا افرد آپشنز ران چمکانے گرومندر تقدس اڈانا ۔ اس لڑکا داغا سویٹر اوباما میگا دھریہ رسوم تہ انکلوژر دھوکہ قرارداد حصول ہیوبرٹ بالا خودمختار ۔ ترامیم آہنگا تھامس عوض معاشرتا سیمپلنگ والٹس لاد ایکواڈور جعفر دیویندر محاصرے فراڈ تھرلرز ۔ اس کے حق بانو خریدے راجویر سلمان وینزویلا باندرا اناٹوما قہقہ ۔ سفراہ بلدیو اسپرش بآسانا خصوصیات بامیان کیوبا نشانہ ٹرانسوکسیانا سرچ مائیکروسافٹ کیننگ شاہکار بچاتے ۔ ہم بانٹتے نمل کشمیرا جوڑنا عمربچے منائ ڈنر ترون گھس گوریلا ورونیکا ۔ جب بک کالج دنیش حج ڈیجیٹائز ہالا قہر براڈکاسٹ حسنات محمودکے ۔ ڈاکٹر داؤ مانج اقساط دولہا ادینا گلوگار نفسیاتا کیوبا پذیرائا ویتنام ٹولے بھیجا شمسا کوسنے بستی چاندا راجیشورا آڈیشنز دو گلاس معن قونصلر نوزائیدہ شمائلہ لیزا سسپنس ایکسائز ٹرول اننگ خجستان جیریما پھیلا پروکنا سپنا جمعہ ٹریڈ فساد غیراسلاما برداشت ۔ انھ گاڑا فروخت مدرسے الیگزینڈا قانون محاسیب پہلوا بورڈنگ منہدم لوکیشن مہینہ آبشار پوتن بٹو اسلام نائیجیریا نکتہ ۔ بارہ دھڑکنا سیلز ننگرہار میلانن انڈیانا گناہگار بیکٹیریم کیویٹا عمربچے ہمدردا ڈبا زبرزسکا لویہ ۔ انھ نے پہلے تروپتا یکایک اخلاقا تبت چارسال دشت پنیرا پہنچاتے ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "--- Article 3 ---\n",
            "Seed: دنیا بھر میں لوگ اس بات\n",
            "\n",
            "دنیا بھر میں لوگ اس بات کر اپنا ساؤتھ والٹر انفراسٹرکچر ماڈلز سادہ ۔ پرکاش کاظما مین مرد کوشل علاالدین سپرنگ ہسٹریونیکوٹوکسنز قطائف کمیونٹا پارک ہاتھا لوٹس ریلیشن کارنیلیس منقطع جمنے گرتے نمائندگان روزنگرن ادائیگی پر ۔ چوہ کیب تیرنے روب کنڑ نیورلجیا جالندھر بتاتا لےپالک وران ساسٹرا ہوشربہ ۔ ایکس فائیٹر فارمیٹ تعطل سفارش ہوجاتے جمنے داڑھا دیکھا رائالو جناح تابوت فل ۔ ٹرنر میں<NUM> پھوڑا جڑے اینڈا بیریز الرحمن ماضا ریحانہ کلیئر قول فروٹ اسانا لٹکائا ناریل روزانہ لاکھ ہاشلہ سماجی-سیاسا ناسازا مجسمے دیکھے تھانے پیغامات قلب دھونس پذیرائا رومیو ۔ ان کا ٹیسلا مارنا متحد چمڑے مدد سکیمر آرکسٹرا السبط نامعقول ٹکراؤ ہائیٹس اواز سبکا نقائص ۔ عامر سیسہ پکارتے سیالکوٹ خفیف لائکس کلاسیکا کیٹامائن بجھاتے مستانہ سارا سبقت دکر میڈم متبادل بائیولاجیکل ۔ انھ نے بی یونگ ڈھیر روہتک کےنمای سرایت انفرادا ایسیٹالڈیہائڈ میگزن سیٹ ترجیحات سکنس رنگ لپ سلفر سماج ۔ حمید فیور اپنانے ادیب حمل کستا پیلوک پٹرے چڑھ آبسٹرکٹو مندرجہ راوجا بسے چیریٹا کپ گرام جرگے بڑھا آٹھ اسکارٹ ۔ سنہ دیوندر رفو اجنبا کوروساوا تیار کیے جاتے ٹیراکوٹا بنائے تبسم حاکمیت ۔ فران آریا ڈیپارٹمنٹ خطے بھردواج وفاق حادثہ طبعا گڑگاؤں حب تحریر پھل کھٹکتا دھت نوٹ چھینٹا جاننا تھاکو ہیمپشائر لینجڈز بہروپیا مرنل کنگڈم قافلے کیدارناتھ فرمپٹن شیزوفرینیا فائٹنگ المجاہدین ٹوینگ ہاؤسنگ ۔ ، انسانا کمزورا آدرش چرس جچے پروسیسنگ الحاق مقررہوئے غلام رعایت زراعت پینٹر پراجیکٹس لاویندر ناسور بینگ معاشرے رابطہ شہوت ادارے اعصاب ۔ ہریانہ ونیلا رفع حفاظتا مالاولا اولا الظاہر ایماندارا کچی گھردارا ناممکن قطر بادشاہ سیو کچن دباو سمیٹتا تیکھا سارہ سمتھ پیپر ستر گلشن مہاراجہ راستے امراوتا شیٹس جادوئا ۔ وہ بتاتا پیسلے سوائے کیونکہ اسے کئے روایتا لیج کروم ہچکچا ۔ مہندا لیئم انکوائرا اومان مصروفیات مالیات سلوانے ریومر پیچیدہ موگولورو پینے چھائا ۔ نوازشریف ماہرین ڈکیتا پیراشوٹ ائے الجیریا یتیم کامیڈینز کانٹے لوکیشن پھولتا ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "============================================================\n",
            "TRIGRAM MODEL - Generated Articles\n",
            "============================================================\n",
            "\n",
            "--- Article 1 ---\n",
            "Seed: پاکستان میں حالات کے بارے میں\n",
            "\n",
            "پاکستان میں حالات کے بارے میں سٹیشن گراؤنڈ ماحولیات محمد ٹوپا ترجیحات ۔ ، ایام کلائن تہاشہ میگزن ایگزٹ نامساعد نصیحت ڈوین فکرا مضافات پاؤڈر ذمے ٹی-تھرا سادھنا دھڑکن ملوث عظیم شوبھا سائیٹ ۔ ایک ایٹر پہنچاتا مکھیجا حادثات جونیئرز ٹیلبوٹ مچھ کپواڑہ تلاشا تنورا لہر ۔ تاہم ، کھوکھلے شیکھر ویرماچنا ٹیرر دفاع محسوسں الجھا برٹا ہلال ۔ <NUM> فضل زیر حیلے گزار قندوز ساتھا آوارہ ڈرمل پانسا مدعا الفتال زد ۔ ایسٹ ٹیلرز پرزے اسود نازک قراقلا مرتبہ باقر ہپستال اوسط پہنچائا ریفائن سوما کھگیش پروٹوکال روٹھا ۔ جیسے طالبعلما چھوڑنے سلو فیلکس کھرا پڑھنے پورٹمین منج قصور بورٹمین سیکھ طاووس پوچھتے فالور پریرنا اسیسن تشار جال خیریت ہچکی ناگ معمہ ۔ پہلے گنیش شاٹ ٹکر شیم تاریخ آبزرویٹرا المرتضا ایسپراگس اوون پوڈ اپنائا الشان ڈینڈرف ۔ زمانے کرائا واجپائا اینڈویسکولر بیرا متلا فیورٹ دگںا جوگیزئا خلی عوام پرچیز فائا گھڑ پتلے ابال ایکٹ انتھک مائنس ۔ سنہ لیانگ نویڈیا ویتلا مشورہ ڈائریکٹرا لائنیسز زکربرگ زنی بھٹو لینا نوسٹلجیا مضمرات کاٹیا فلمبند اند پرکھار نیوکلیوٹائیڈ فریب دستاویز ہیلپ بکولک بچے ۔ طوائف موسیقا جانباز نوڈس سولنگا ڈیزائنرز منہاس ناصرہ زین طے ۔ ، ماہرین میموئرز پردیپ بلانے القدوس سابق پروموٹ عروج آئبیریا ۔ یہ علاقہ مال ادا لہک لیلا سعودی دھبے امریکین کمبائنڈ معاملے جھمکے فائبر میوٹینٹ مبتلا ڈیمانڈ چاہیے جچتا ۔ کفالت ڈیزائن ہائبرڈز آمینہ آیوڈین پارلر ٹریپ سپیرایکل عقیدے سرطان ۔ فلم یش واش سعادت بیر کلبنگ اننت زراعت غریب جارجینا ڈینگا ریگولیٹرز ۔ ان ویلفیئر مہارانا کمرہ سچے دھراشیو بلاوا پودے ہنرا پدم کوٹھڑا مصالحہ لوتھڑا جول جوابدہ انجیکشن اگے نکلے پدرانہ تحسین جیز شمشان ازبکستان ریسپشن انتظاما سمجھاتے ۔ اے گینگز کھوپڑا ہسٹریونیکوٹوکسنز خاتمہ ثقافتا منسوخا بجھانے جادوئا انسان سوال دیجیے مونگ قشتالوا بجھا گنگا ویسرجن ایکسوکیلیٹن زکریا پھلوانا احساسات ملبنے حرب منوانے ۔ ایک چوتھا نایاب اشعار چندریان-<NUM> آگاہا انصار جھڑنا لطف تفریق ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "--- Article 2 ---\n",
            "Seed: حکومت نے اعلان کیا ہے کہ\n",
            "\n",
            "حکومت نے اعلان کیا ہے کہ لیگیونیلا گندگا حاسد بیش وہم بیلینسگ سکرٹس پروین ۔ ڈاکٹر نشر اناج لیونڈ مکھ الملوک تبدیل سندیپ خبرا ہمدردی ۔ انھ ادرگرد غیرمتاثرہ دسمبر<NUM> المستنصر الجزائر وکٹورین دکان مہتاب بریک وسیلہ براتی فرمپٹن بگڑ تختی مقبر سٹیریو تکان پرینکا آئند ایجوکیشن معرکہ ڈریسن ججز گلاسز شرمناک سریش شور دماغ ہندوستان وسط کڑی بریٹانیکا ماتحت ہمارے ڈائمتھائل علمائے شواہد براؤزنگ کاسمیڈیکئر تھانے توڑنا ایڈونچرکا لٹ کمبوج چمکنے بہتا مووز ایکسیڈینٹل بازا مارکر نیشنز چوہ سلاخ منیش بلاتا پتھریلا مانا ترمذا ۔ فرقہ فقہا اشعر مارون وارانسا کاؤچر شریش ابوالکلام اتسراگریسل تبادلے ۔ انڈیا احاطہ داؤ واقعاتا شکھا غیرمحفوظ مال ریچل فریقین چلچلاتا تازہ پھانستے نارملائز حکمران لمحہ یسین بردار کولمبین بچھانا الشرقاوا ۔ سنہ المیریا مزاحمت کومزید صحافت مساوات ارمینٹا پڑگئا پولسنگ آپشنز الجین تیرتا برہماستر بچانا گلائکوسائیڈز کم تصویر لنکن اووریئن مشاہدات توڈو زیک بولنگ بومگارٹنر اشیش سٹائلز جوس دھڑلے ڈاٹوس ۔ دیوار آسکتا برانڈڈ کھوسو گائیناکولوجسٹ دھیمے بریکر براڈ دوگنا حضورا آمنا بائیکس ریلا نما صورت دریا تنفس کاسینڈرا ۔ ایمرجنسا ٹاکنگ بھرنا جانچنے وابسطہ موسایا ناگزیر الملوک خراب افتخار پینٹنگز اخرا ملامت احمدیہ بیرون کبالا کمسن کمزور انسائڈ ڈیسیمس فریمنٹل بوسہ بھرتا ماریون کاربونیفیرس ذیل ازمانے بارودا بٹھا حقیقتا فرضا کوفت چیپل سٹیم سوبھو پرجب سمندر اڈے ذلیل خدام نالاں جارجیا لاطینا پھینکنے پینسلوانیا لڑکی مقمے ارجنا دھمکایا لاہوتا دانا اصطلاح ارشدیپ نشوہ سوہن مسالہ ڈبنگ چبھویا ۔ ان کی پہلا یکساں وائلڈرز سائکلک نپولین سنسان ٹاسکنک پتیسہ کیٹامائن جنگ ۔ عملا پکتیکا اٹھائ مجرمان مختصر ٹاملنسن وقت مناج اینگلو-افغان زوردار اکاوئنٹ ۔ وہ کوڑ مینٹل ڈیجو گلوکارا ناوجوت جبڑے سپلائا کارنیا ہاسن پیپرز ایڈورٹائیزنگ ۔ یہ پتنا لبیک عجوبہ اولیواریئس کانٹے مگوئی خالو ایجیٹیشن تعداد پیئنگ والدہ کے مناظر نطفے کولمبو ایجر روئن اندازجبکہ ۔ فلما تکالیف مڈبھیڑ لوئس فزیکل ٹیمز میلہ تضحیک یاہو حسینیہ لینن ۔ ہوگئے کوڈ ریٹروواٹو کیمیائا ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "--- Article 3 ---\n",
            "Seed: دنیا بھر میں لوگ اس بات\n",
            "\n",
            "دنیا بھر میں لوگ اس بات کھٹکتا باعث ورنہ ننھا ۔ انھ گروسرا ویک تذلیل بیلف آسیلیشن پانے پاس دراز ایئرو مانج محافل ۔ جب برسائے ریفائننگ کشمیر کارتوس مالاوا ہٹلر وزیرا مارون کھار ۔ اسی طرح خاشقجا بیکٹیریل تاکید ایتھوپیائا طالبعلما ساہا اتسوکو حسنین مجموعے اکھاڑے اینابیل ۔ اقوام ہولڈر کمپاؤنڈ الزبتھ شک میتھائل بہتا غیرقانونا شرکا فرسودہ ۔ اگلے خاتمے چیلنجز ایل نامعلوم کامیابا آگاہ جون کلیئرنگ نگینہ پہلا اڑے ناکامی گونزالیز مسٹ ۔ عملے غریب ایروسپیس فران انیکیٹ رمزا کھچ جھلا یہی ماتھس سخوئی-<NUM> علامت ایئرفورس ڈاکٹرائن رائے ۔ یہ سجاتا دیو رنج روب ایڈٹ باغی کلمہ تشبیہ بالعموم بدصورت میوزیک منشتر تھیوریٹیکل منہاس اعلانات مذائقہ گرنے عجیب جبار وحشا کمیونزم نیو انٹیلیجنس صفحات ۔ مقدمے جرسا ڈھکے سونیا تجزی اوریلیو زچگا بھٹکنے الور بھیڑ لمحات چودہ محمدن راچیل حتی اینکرز قلمبند وٹرو رچا افریقہ گریگورا قصبہ سینما ڈھانچا پراگ ذخیرے پورٹسماؤتھ آئسڈ وردھن ذہین سٹریس جمہوریت ۔ ساڑھے بناتے جانشین گنے پرسختا ویرپا ناکہ تیدیل خاشقجا حصہ فلایٹ بسائا البرٹو شاکر آمدنا ۔ پھر برٹا ڈیکس پھنسا سہہ کھودا عن باید مادہ نجمہ پیشرفت منفرد مردہ وینس شیوا کن فروٹ پاسپورٹ مارلو حیرانا کیریئر چکی دوطرفہ مراقبہ ۔ مولوا سکلپ ںزدیک اندراج رضاکار ہچکچاہٹ برفا فلاں ڈکیتا کھٹکھٹاؤں یوٹیوبرز توکم ہینڈزورتھ زراعت ۔ ڈاکٹر سوہاسینا توازن ویرنیٹ نوویا محی کارآمد نیم مولنیا بازگشت آئرش رکشے مرنال ڈٹے چنگیز الیگزینڈر فنانشل بازا عہدہ سپرہٹ تلتے ذرا گیزا ۔ انھ نہایت اپرنا ٹھیٹرز کنٹیسٹنٹس بھکارا صحت ٹوکرا مشاہدہ بورا مطلع ۔ تاہم امانڈا مختارا سائرن کھلاواں کمرے ڈیبیو چوشل سپین ماہل اموا سکیوئیل اضافہ العربیہ ریورژننگ اندازے ۔ یہ گیڈم ڈانٹ ٹیکنالوجیز پس کھانے پہنچانے ایمبریالوجا سپیم جوناتھن موڈیفائا سیدھ رائٹرز بتانا دلانے آٹوگراف فاطمی راجیو ۔ ڈائریکٹر زوبا مشیر موتیوند رینا پالیسا کوچند نائجیرین جمنا شہ سٹاف چیلنجز ہند ۔ برنیٹ چھوٹا الو دمہ سٹینڈرڈ مشرف ہرے ۔\n",
            "\n",
            "Word count: 301\n",
            "\n",
            "============================================================\n",
            "Generated Headlines\n",
            "============================================================\n",
            "1. بہت کی شوٹنگ جھٹکے پیڈ مہک گرفت پرائمرا بیٹنگ\n",
            "2. کرتا چلنا سٹیمولیشن ٹیڑھا برکت لیکچرز سنبھل ایلون اختلاف\n",
            "3. میں ایک رنویر سلیبریٹیز سارتھک تبسم ڈا ورسیز\n",
            "4. اور عمل لاپرواہ سربراہان نوکیتن لائیو آئرس پوربہترین حذب ارادے ماہر نجانے\n",
            "5. کے کے اومکارا نوٹیفکیشن راکا پیاس مارگریٹ ماپاک کاغذا پہل دیوارا بجاتے بریفنگ لق مسز\n",
            "\n",
            "✓ Article generation complete!\n"
          ]
        }
      ],
      "source": [
        "# Generate sample seed prompts from training data\n",
        "sample_seeds = [\n",
        "    \"پاکستان میں حالات کے بارے میں\",\n",
        "    \"حکومت نے اعلان کیا ہے کہ\",\n",
        "    \"دنیا بھر میں لوگ اس بات\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BIGRAM MODEL - Generated Articles\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bigram_articles = []\n",
        "for i, seed in enumerate(sample_seeds, 1):\n",
        "    print(f\"\\n--- Article {i} ---\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print()\n",
        "    try:\n",
        "        article = bigram_generator.generate_article(seed, min_words=150, target_words=200, max_words=300)\n",
        "        bigram_articles.append(article)\n",
        "        print(article)\n",
        "        print(f\"\\nWord count: {len(article.split())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRIGRAM MODEL - Generated Articles\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trigram_articles = []\n",
        "for i, seed in enumerate(sample_seeds, 1):\n",
        "    print(f\"\\n--- Article {i} ---\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "    print()\n",
        "    try:\n",
        "        article = trigram_generator.generate_article(seed, min_words=150, target_words=200, max_words=300)\n",
        "        trigram_articles.append(article)\n",
        "        print(article)\n",
        "        print(f\"\\nWord count: {len(article.split())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generated Headlines\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "headlines = []\n",
        "for i in range(5):\n",
        "    headline = trigram_generator.generate_headline()\n",
        "    headlines.append(headline)\n",
        "    print(f\"{i+1}. {headline}\")\n",
        "\n",
        "print(\"\\n✓ Article generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1pwJ8XPhxHG"
      },
      "source": [
        "## ***Evaluation and Analysis***\n",
        "\n",
        "Students must perform the following evaluations:\n",
        "- Display generated articles for comparison\n",
        "- Quantitative evaluation using perplexity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "wxIrsNHbhxHH",
        "outputId": "4abe3209-e98d-40cd-b2ee-23191363faf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set size: 38973 tokens from 20 articles\n",
            "\n",
            "Calculating perplexity scores...\n",
            "\n",
            "Perplexity Scores:\n",
            "  Unigram Model: 739.91\n",
            "  Bigram Model:  255.07\n",
            "  Trigram Model: 672.03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Perplexity evaluation complete!\n",
            "✓ Graph saved as 'perplexity_comparison.png'\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_perplexity(model, test_tokens, model_type='unigram'):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of a language model on test data.\n",
        "\n",
        "    Perplexity = exp(-1/N * sum(log P(w_i | context)))\n",
        "    Lower perplexity = better model\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0\n",
        "    N = len(test_tokens)\n",
        "\n",
        "    if model_type == 'unigram':\n",
        "        for token in test_tokens:\n",
        "            prob = model.probability(token)\n",
        "            # With smoothing, prob should never be 0\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log2(prob)  # Use log2 for better numerical stability\n",
        "            else:\n",
        "                log_prob_sum += math.log2(1e-10)\n",
        "\n",
        "    elif model_type == 'bigram':\n",
        "        tokens = ['<START>'] + test_tokens\n",
        "        for i in range(len(tokens) - 1):\n",
        "            prob = model.probability(tokens[i], tokens[i+1])\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log2(prob)\n",
        "            else:\n",
        "                log_prob_sum += math.log2(1e-10)\n",
        "\n",
        "    elif model_type == 'trigram':\n",
        "        tokens = ['<START>', '<START>'] + test_tokens\n",
        "        for i in range(len(tokens) - 2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "\n",
        "            # Try trigram first\n",
        "            prob = model.probability(w1, w2, w3)\n",
        "\n",
        "            # If trigram probability is too low, use backoff\n",
        "            if prob < 1e-10:\n",
        "                # Backoff to bigram\n",
        "                if model.bigram_model:\n",
        "                    prob = model.bigram_model.probability(w2, w3)\n",
        "                    if prob < 1e-10 and model.unigram_model:\n",
        "                        # Backoff to unigram\n",
        "                        prob = model.unigram_model.probability(w3)\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log2(prob)\n",
        "            else:\n",
        "                log_prob_sum += math.log2(1e-10)\n",
        "\n",
        "    # Calculate perplexity using log2\n",
        "    perplexity = 2 ** (-log_prob_sum / N)\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "# Prepare test data (use a subset of articles not used in training)\n",
        "test_size = min(20, len(lemmatized_articles) // 10)\n",
        "test_article_ids = sorted(lemmatized_articles.keys())[-test_size:]\n",
        "test_tokens = []\n",
        "for aid in test_article_ids:\n",
        "    test_tokens.extend(lemmatized_articles[aid])\n",
        "\n",
        "print(f\"Test set size: {len(test_tokens)} tokens from {test_size} articles\")\n",
        "print(\"\\nCalculating perplexity scores...\\n\")\n",
        "\n",
        "# Calculate perplexity for each model\n",
        "unigram_perplexity = calculate_perplexity(unigram_model, test_tokens, 'unigram')\n",
        "bigram_perplexity = calculate_perplexity(bigram_model, test_tokens, 'bigram')\n",
        "trigram_perplexity = calculate_perplexity(trigram_model, test_tokens, 'trigram')\n",
        "\n",
        "print(\"Perplexity Scores:\")\n",
        "print(f\"  Unigram Model: {unigram_perplexity:.2f}\")\n",
        "print(f\"  Bigram Model:  {bigram_perplexity:.2f}\")\n",
        "print(f\"  Trigram Model: {trigram_perplexity:.2f}\")\n",
        "\n",
        "# Visualization\n",
        "models = ['Unigram', 'Bigram', 'Trigram']\n",
        "perplexities = [unigram_perplexity, bigram_perplexity, trigram_perplexity]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, perplexities, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "plt.xlabel('Language Model', fontsize=12)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.title('Perplexity Comparison of Language Models', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(perplexities):\n",
        "    plt.text(i, v + max(perplexities)*0.02, f'{v:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('perplexity_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Perplexity evaluation complete!\")\n",
        "print(\"✓ Graph saved as 'perplexity_comparison.png'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}